from __future__ import annotations

import json
from dataclasses import dataclass, field
from math import ceil, floor
from typing import List, Literal, Optional, TypedDict, Any, Union

from pydantic import BaseModel

MessageRole = Literal["system", "user", "assistant", "function"]
MessageType = Literal["ai_response", "action_result"]

TText = list[int]
"""Token array representing tokenized text"""


class MessageDict(TypedDict):
    role: MessageRole
    content: str


@dataclass
class Message:
    """OpenAI Message object containing a role and the message content"""

    role: MessageRole
    content: str
    function_call: OpenAIFunctionCall | None = None

    def raw(self) -> MessageDict:
        return {"role": self.role, "content": self.content}


@dataclass
class MessageFunctionCall:
    role: MessageRole
    name: str
    content: MessageType | None = None

    def raw(self) -> MessageDict:
        return {"role": self.role, "name": self.name, "content": json.dumps(self.content)}


@dataclass
class ModelInfo:
    """Struct for model information.

    Would be lovely to eventually get this directly from APIs, but needs to be scraped from
    websites for now.
    """

    name: str
    max_tokens: int
    prompt_token_cost: float


@dataclass
class CompletionModelInfo(ModelInfo):
    """Struct for generic completion model information."""

    completion_token_cost: float


@dataclass
class ChatModelInfo(CompletionModelInfo):
    """Struct for chat model information."""
    TPM: int
    RPM: int


@dataclass
class TextModelInfo(CompletionModelInfo):
    """Struct for text completion model information."""


@dataclass
class EmbeddingModelInfo(ModelInfo):
    """Struct for embedding model information."""

    embedding_dimensions: int


@dataclass
class ChatSequence:
    """Utility container for a chat sequence"""

    model: ChatModelInfo
    messages: list[Message] = field(default_factory=list)

    def __getitem__(self, i: int):
        return self.messages[i]

    def __iter__(self):
        return iter(self.messages)

    def __len__(self):
        return len(self.messages)

    def append(self, message: Message):
        return self.messages.append(message)

    def extend(self, messages: list[Message] | ChatSequence):
        return self.messages.extend(messages)

    def insert(self, index: int, *messages: Message):
        for message in reversed(messages):
            self.messages.insert(index, message)

    @classmethod
    def for_model(cls, model_name: str, messages: list[Message] | ChatSequence = []):
        from utils.llm.openai_utils import OPEN_AI_CHAT_MODELS

        if not model_name in OPEN_AI_CHAT_MODELS:
            raise ValueError(f"Unknown chat model '{model_name}'")

        return ChatSequence(
            model=OPEN_AI_CHAT_MODELS[model_name], messages=list(messages)
        )

    def add(self, message_role: MessageRole, content: str):
        self.messages.append(Message(message_role, content))

    @property
    def token_length(self):
        from utils.llm.token_counter import count_message_tokens

        return count_message_tokens(self.messages, self.model.name)

    def raw(self) -> list[MessageDict]:
        return [m.raw() for m in self.messages]

    def dump(self) -> str:
        SEPARATOR_LENGTH = 42

        def separator(text: str):
            half_sep_len = (SEPARATOR_LENGTH - 2 - len(text)) / 2
            return f"{floor(half_sep_len) * '-'} {text.upper()} {ceil(half_sep_len) * '-'}"

        formatted_messages = "\n".join(
            [f"{separator(m.role)}\n{m.content}" for m in self.messages]
        )
        return f"""
============== ChatSequence ==============
Length: {self.token_length} tokens; {len(self.messages)} messages
{formatted_messages}
==========================================
"""


@dataclass
class LLMResponse:
    """Standard response struct for a response from an LLM model."""

    model_info: ModelInfo
    prompt_tokens_usage: int = 0
    completion_tokens_usage: int = 0


@dataclass
class EmbeddingModelResponse(LLMResponse):
    """Standard response struct for a response from an embedding model."""

    embedding: List[float] = field(default_factory=list)

    def __post_init__(self):
        if self.completion_tokens_used:
            raise ValueError("Embeddings should not have completion tokens used.")


@dataclass
class OpenAIFunctionCall:
    """Represents a function call as generated by an OpenAI model

    Attributes:
        name: the name of the function that the LLM wants to call
        arguments: a stringified JSON object (unverified) containing `arg: value` pairs
    """

    name: str
    arguments: str

    def raw(self) -> dict[str, str]:
        return {"name": self.name, "arguments": self.arguments}


@dataclass
class ChatModelResponse(LLMResponse):
    """Standard response struct for a response from an LLM model."""
    role: str = "student"
    data_id: str = ""
    content: Optional[str] = None
    function_call: Union[EntityAgentResponse] = None


@dataclass
class EntityAgentResponse:
    tokens: List[str]
    ner_tags: List[str]
    data_id: Union[str, None] = None

    def raw(self) -> MessageDict:
        return {"id": self.data_id, "tokens": self.tokens, "ner_tags": self.ner_tags}

    def __post_init__(self):
        if len(self.tokens) != len(self.ner_tags):
            raise ValueError("Tokens and NER tags must be the same length")

# def get_example(dataset, id_to_label, function_name="find_ner"):
#     from functools import partial
#
#     examples = []
#     encode_fn = partial(make_example, id_to_label=id_to_label)
#     entities = dataset.map(encode_fn)
#     for entity_ex in entities:
#         entities = entity_ex["entities"]
#         tokens = str(entity_ex["tokens"])
#         function_example = OpenAIFunctionCall(name=function_name,
#                                               arguments=json.dumps({"entities": entities}))
#         examples.append(MessageFunctionCall("assistant", tokens, function_example))
#
#     return examples
